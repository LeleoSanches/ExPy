{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# NLP - Natural Language Processing <h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. spaCy\n",
    "![](architecture-bcdfffe5c0b9f221a2f6607f96ca0e4a.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As estruturas centrais dos dados no _spaCy_ são **Doc** e **Vocab**.\n",
    "\n",
    "> **Doc** contém a sequencia de **tokens** e todas suas anotações\n",
    "\n",
    "> **Vocab** contém _look-up_ _tables_ que faz uma informação comum entre todo documento\n",
    "\n",
    "Centralizando _strings_, _word_ _vectors_ e atributos _lexical_, avita-se guardar multiplas cópias dos dados.\n",
    "\n",
    "> O Objeto **Doc** é construido pelo **Tokenizer**, e então modificado por componentes do pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. Container objects\n",
    "\n",
    "> **Doc**  - Um container para acessar anotaçõess linguisticas\n",
    "\n",
    "> **Span** - Um pedaço do objeto **Doc**\n",
    "\n",
    "> **Token** - Um _token_ individual (palavra, pontuação, espaço, etc.)\n",
    "\n",
    "> **Lexeme** - Uma entrada no vocabulario. Uma palavra sem contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 1.2. Processing Pipeline\n",
    "\n",
    "> **Language** - Pipeline de processamento textual. Carrega uma vez por processo e passa a instancia sobre a aplicação\n",
    "\n",
    "> **Tokenizer** - Segmenta o text e cria objetos **Doc**\n",
    "\n",
    "> **Lemmatizer** - Determina a forma base das palavras\n",
    "\n",
    "> **Tagger** - Cria Tags de **part-of-speech** em **Doc**\n",
    "\n",
    "> **DependencyParser** - Anota dependencias sintaticas em **Doc**\n",
    "\n",
    "> **EntityRecognizer** - Anota Nomes de entidades em **Doc**\n",
    "\n",
    "> **TextCategorizer** - Carrega categorias ou labels em **Doc**\n",
    "\n",
    "> **Matcher** - Combina uma sequência de tokens, baseado em padrões\n",
    "\n",
    "> **PhraseMatcher** - Combina sequência de tokens baseado em frases\n",
    "\n",
    "> **EntityRuler** - Adiciona _spans_ de endidades ao **Doc** usando regras _token_ ou uma frase\n",
    "\n",
    "> **Sentencizer** - Implementa uma logica de detecção de sentenças que não requer _dependency_ _parse_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Outras Classes\n",
    "\n",
    "> **Vocab** - Uma tabela para acessar objetos **Lexeme**\n",
    "\n",
    "> **StringStore** - Mapeia strings para _hash_ _values_\n",
    "\n",
    "> **Vectors** Conteiner class for vector data keyed by string\n",
    "\n",
    "> **GoldParse** - Coleção para _training_ _annotation__\n",
    "\n",
    "> **GoldCorpus** - An annotated corpus, using the JSON file format. Manages annotations for tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 2. Linguistic Features\n",
    "\n",
    "## 2.1 Tokenization\n",
    "\n",
    "Tokenizar significa fazer o splitting de um texto em segmentos com significados (**Tokens**)\n",
    "\n",
    "O input do __Tokenizer__ é um texto unicode, a saída é um objeto __Doc__. Para construir um __Doc__, é necessário uma instância __Vocab__, uma sequencia de strings e, opcionalmente, uma sequencia de __spaces__ booleans, que permitem a manutenção entre __tokens__ na string original.\n",
    "\n",
    "> A _tokenização_ realizada pelo spaCy é não-destrutiva, ou seja, é possivel sempre reconstruir o input original do output tokenizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu\n",
      "duvido\n",
      "que\n",
      "você\n",
      "tokeniza\n",
      "essa\n",
      "frase\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "doc = nlp('Eu duvido que você tokeniza essa frase')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Processo de tokenizacao](tokenization-57e618bd79d933c4ccd308b5739062d6.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Adicionando Regras de Tokenizacao\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eu', 'duvido', 'que', 'você', 'tokeniza', 'essa', 'frase']\n",
      "['toke', 'niza', 'essa', 'frase']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "doc = nlp('Eu duvido que você tokeniza essa frase')\n",
    "\n",
    "print([w.text for w in doc])\n",
    "\n",
    "caso_especial = [{ORTH: 'toke'}, {ORTH: 'niza'}]\n",
    "nlp.tokenizer.add_special_case('tokeniza', caso_especial)\n",
    "\n",
    "print([w.text for w in nlp('tokeniza essa frase')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Customizando a classe Tokenizer\n",
    "\n",
    "Para criar um Tokenizer é necessário:\n",
    "1.  Um dicionario de Casos Especiais\n",
    "2. Uma função de __prefix_search__, para lidar com pontuações\n",
    "3. Uma função de __suffix_search__, para lidar com pontuações sucessivas\n",
    "4. Uma função de __infixes_finditer__, para lidar com separações que não são por espaço\n",
    "5. Uma função __token_match__, para combinar strings que nunca farão split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Modificando regras existentes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As regras de __prefix__, __suffix__ e __infix__ podem ser modificadas.\n",
    "\n",
    "> Tokenizer.suffix_search - Pode-se adicionar/remover regras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = nlp.Defaults.suffixes + (r'''-+$''',)\n",
    "#print(suffixes)\n",
    "suffix_regex = spacy.util.compile_suffix_regex(suffixes)\n",
    "nlp.tokenizer.suffix_search = suffix_regex.search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = list(nlp.Defaults.suffixes)\n",
    "suffixes.remove('\\\\[')\n",
    "suffix_regex = spacy.util.compile_suffix_regex(suffixes)\n",
    "nlp.tokenizer.suffix_search = suffix_regex.search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Sentece Segmentation\n",
    "\n",
    "__spaCy__ utiliza análise de dependência para determinar os limites das sentenças. Isso significa que é necessário uma Modelagem Estatística.\n",
    "\n",
    "### 2.2.1. Padrão: dependency parse\n",
    "\n",
    "Para ver as sentenças de um __Doc__ usa-se __Doc.sents__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu duvido que você tokeniza essa frase.\n",
      "Essa daqui nem se fala\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "doc = nlp('Eu duvido que você tokeniza essa frase. Essa daqui nem se fala')\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Rule-based pipeline component\n",
    "\n",
    "O componente __Sentencizer__ é um pipeline que splita as pontuações das sentenças.\n",
    "\n",
    "Pode-se conectar no pipeline se precisar apenas da sentença entre os limites, sem realizar dependency parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "sentencizer = nlp.create_pipe('sentencizer')\n",
    "nlp.add_pipe(sentencizer)\n",
    "doc = nlp('This is a sentence. This is another sentence.')\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Custom rule-based strategy\n",
    "\n",
    "Pode-se criar um pipeline customizado que pega o __Doc__ e configura o atributo __Token.is_sent_start__ em cada token individual.\n",
    "\n",
    "> Pode-se apenas configurar as limites antes de fazer o parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes:  ['Isso daqui é uma frase~~ eita outra frase ~~ isso daqui', 'é outra frase']\n",
      "Depois:  ['Isso daqui é uma frase~~ eita outra frase ~~ isso daqui', 'é outra frase']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "texto = 'Isso daqui é uma frase~~ eita outra frase ~~ isso daqui é outra frase'\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "doc = nlp(texto)\n",
    "print('Antes: ', [sent.text for sent in doc.sents])\n",
    "\n",
    "def customiza_bondaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == '~~~':\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(customiza_bondaries, before='parser')\n",
    "doc = nlp(texto)\n",
    "print('Depois: ', [sent.text for sent in doc.sents])\n",
    "\n",
    "# naofunciona em portugues e nao sei pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Combinação por regras (Token)\n",
    "\n",
    "* Encontrar Frases, tokens e entidades\n",
    "\n",
    "Rule_based matching possibilita encontrar palavras e frases dando acesso aos tokens e suas relações\n",
    "\n",
    "### 2.3.1 Regras ou treinar um modelo\n",
    "\n",
    "1. Para tarefas complexas, usualmente é melhor treinar um modelo estatístico de reconhecimento. Porém, modelos estatísticos requerem dados de treinamento.\n",
    "2. Pode-se começar um projeto utilizando rule-based para ajudar a coletar os dados\n",
    "3. Treinar modelo é util se há exemplos que você quer que seu sistema seja apto a generalizar.\n",
    "\n",
    "## 2.3.2 Matching baseado em Token\n",
    "\n",
    "o __Matcher__, que opera em __tokens__, serve como uma operação de rule-matching. As regras podem ser referidas por anotações em tokens. \n",
    "\n",
    "Para adicionar padrões:\n",
    "\n",
    "1. Inicializar o __Matcher__ com um __vocab__.\n",
    "2. Add __matcher.add()__ com um ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "951075153562793782 OiMundo 0 3 Oi, mundo\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'LOWER': 'oi'}, {'IS_PUNCT': True}, {'LOWER': 'mundo'}]\n",
    "matcher.add('OiMundo', None, pattern)\n",
    "\n",
    "doc = nlp('Oi, mundo! Oi mundo')\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Matcher retorna uma lista que mapeia o span em __doc[0:3]__\n",
    "\n",
    "![](Captura.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Combinação por regra (Frases)\n",
    "\n",
    "Usa-se __PhraseMatcher__ que cria um __Doc__ contendo multiplos tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angela Merkel\n",
      "Barack Obama\n",
      "Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
    "# Only run nlp.make_doc to speed things up\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", None, *patterns)\n",
    "\n",
    "doc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "          \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar patterns, cada frase deve ser processada com nlp. Se o modelo carregado faz um loop ou um list comprehension, pode ser ineficiente e lento.\n",
    "\n",
    "Caso precisa-se apenas da tokenização e dos atributos Lexical, utiliza-se __nlp.make_doc__\n",
    "\n",
    "\n",
    "### Combinação por formato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched based on token shape: 192.168.1.1\n",
      "Matched based on token shape: 192.168.2.1\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='Shape')\n",
    "matcher.add('IP', None, nlp('127.0.0.1'), nlp('127.127.0.0'))\n",
    "\n",
    "doc = nlp(\"Often the router will have an IP address such as 192.168.1.1 or 192.168.2.1.\")\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(\"Matched based on token shape:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Combinação por reconhecimento de Entidade\n",
    "\n",
    "O __EntityRuler__ é capaz de adicionar nome de entidades baseados em padrões de um dicionario.\n",
    "\n",
    "> **Phrase patterns** para matches exatos: {'label': 'ORG', 'pattern': 'SENAI}\n",
    "\n",
    "> **Token patterns** com um dicionario que descreve o token: {'label' : 'CIDADE', 'pattern': [{'LOWER':' 'Londrina'}, {'LOWER':'Ibipora'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SENAI', 'ORG'), ('Londrina', 'CIDADE'), ('Ibipora', 'CIDADE')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = English()\n",
    "ruler = EntityRuler(nlp)\n",
    "patterns = [{'label': 'ORG', 'pattern': 'SENAI'}, {'label' : 'CIDADE', 'pattern': 'Londrina'}, {'label' : 'CIDADE','pattern':'Ibipora'}]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "doc = nlp('Couto gonna open a SENAI in Londrina and Ibipora')\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
